# LM Light åˆ©ç”¨ãƒãƒ‹ãƒ¥ã‚¢ãƒ« 

## Ollamaãƒãƒ¼ã‚¸ãƒ§ãƒ³

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« | ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ 

### macOS

```bash
curl -fsSL https://raw.githubusercontent.com/lmlight-app/dist_v3/main/scripts/install-macos.sh | bash
```

### Linux

```bash
curl -fsSL https://raw.githubusercontent.com/lmlight-app/dist_v3/main/scripts/install-linux.sh | bash
```

### Windows

```powershell
irm https://raw.githubusercontent.com/lmlight-app/dist_v3/main/scripts/install-windows.ps1 | iex
```

---

ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å…ˆ:
- macOS/Linux: `~/.local/lmlight`
- Windows: `%LOCALAPPDATA%\lmlight`

**Docker:** [Dockerç‰ˆ](#dockerç‰ˆ) ã‚’å‚ç…§

## ç’°å¢ƒæ§‹ç¯‰ (ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å‰ã«å®Ÿè¡Œ)

### å¿…è¦ãªä¾å­˜é–¢ä¿‚

| ä¾å­˜é–¢ä¿‚ | macOS | Linux (Ubuntu/Debian) | Windows |
|---------|-------|----------------------|---------|
| Node.js 18+ | `brew install node` | `sudo apt install nodejs npm` | `winget install OpenJS.NodeJS.LTS` |
| PostgreSQL 17 | `brew install postgresql@17` | `sudo apt install postgresql` | `winget install PostgreSQL.PostgreSQL.17` |
| pgvector | `brew install pgvector` | `sudo apt install postgresql-17-pgvector` | [æ‰‹å‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«](https://github.com/pgvector/pgvector#windows) |
| Ollama | `brew install ollama` | `curl -fsSL https://ollama.com/install.sh \| sh` | `winget install Ollama.Ollama` |
| FFmpeg (æ–‡å­—èµ·ã“ã—ç”¨) | `brew install ffmpeg` | `sudo apt install ffmpeg` | `winget install Gyan.FFmpeg` |
| Tesseract OCR  | `brew install tesseract` | `sudo apt install tesseract-ocr` | `winget install UB-Mannheim.TesseractOCR` |

### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹

ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ©ãƒ¼ãŒDBä½œæˆãƒ»ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆãƒ»åˆæœŸãƒ¦ãƒ¼ã‚¶ãƒ¼ä½œæˆã‚’è‡ªå‹•å®Ÿè¡Œã—ã¾ã™ã€‚

ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã¿ã‚’æ‰‹å‹•å®Ÿè¡Œ:

```bash
# macOS/Linux
curl -fsSL https://raw.githubusercontent.com/lmlight-app/dist_v3/main/scripts/db_setup.sh | bash
```

```powershell
# Windows
irm https://raw.githubusercontent.com/lmlight-app/dist_v3/main/scripts/db_setup.ps1 | iex
```

**ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å‰Šé™¤:**
```bash
psql -U postgres -c "DROP DATABASE lmlight;"
# ãã®å¾Œã€ä¸Šè¨˜ã®db_setupã‚’å†å®Ÿè¡Œ
```

â€» ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆæ™‚ã‚‚æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¯ä¿æŒã•ã‚Œã¾ã™

### Ollamaãƒ¢ãƒ‡ãƒ«

[Ollama ãƒ¢ãƒ‡ãƒ«ä¸€è¦§](https://ollama.com/search) ã‹ã‚‰å¥½ã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ:

```bash
ollama pull <model_name>        # ä¾‹: gemma3:4b, llama3.2, qwen2.5 ãªã©
ollama pull nomic-embed-text    # RAGç”¨åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ« (æ¨å¥¨)
```

### è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« (.env)

ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¾Œã€`.env` ã‚’ç·¨é›†:
- macOS/Linux: `~/.local/lmlight/.env`
- Windows: `%LOCALAPPDATA%\lmlight\.env`

| ç’°å¢ƒå¤‰æ•° | èª¬æ˜ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ |
|---------|------|-----------|
| `DATABASE_URL` | PostgreSQLæ¥ç¶šURL | `postgresql://lmlight:lmlight@localhost:5432/lmlight` |
| `OLLAMA_BASE_URL` | Ollamaã‚µãƒ¼ãƒãƒ¼URL | `http://localhost:11434` |
| `LICENSE_FILE_PATH` | ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ | `~/.local/lmlight/license.lic` |
| `API_HOST` | APIãƒã‚¤ãƒ³ãƒ‰ã‚¢ãƒ‰ãƒ¬ã‚¹ | `0.0.0.0` (å…¨ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹) |
| `API_PORT` | APIãƒãƒ¼ãƒˆ | `8000` |
| `WEB_HOST` | Webãƒã‚¤ãƒ³ãƒ‰ã‚¢ãƒ‰ãƒ¬ã‚¹ | `0.0.0.0` (å…¨ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹) |
| `WEB_PORT` | Webãƒãƒ¼ãƒˆ | `3000` |

**ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å®š:**
- `API_HOST` / `WEB_HOST` ã‚’ `0.0.0.0` ã«è¨­å®šã™ã‚‹ã¨ã€åŒã˜LANå†…ã®ä»–ã®PCã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã«ãªã‚Šã¾ã™
- `127.0.0.1` ã«è¨­å®šã™ã‚‹ã¨ã€åŒã˜ãƒã‚·ãƒ³ã‹ã‚‰ã®ã¿ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã«ãªã‚Šã¾ã™ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–ï¼‰

â€» ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ©ãƒ¼ãŒè‡ªå‹•è¨­å®šã—ã¾ã™ã€‚æ‰‹å‹•å¤‰æ›´ãŒå¿…è¦ãªå ´åˆã®ã¿ç·¨é›†ã—ã¦ãã ã•ã„ã€‚
â€» NEXTAUTH_SECRETç­‰ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é–¢é€£è¨­å®šã¯ã‚¢ãƒ—ãƒªå†…è”µã®ãŸã‚ã€.envã§ã®è¨­å®šã¯ä¸è¦ã§ã™ã€‚

### æ–‡å­—èµ·ã“ã—æ©Ÿèƒ½ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)

éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹æ©Ÿèƒ½ã§ã™ã€‚è©³ç´°ã¯ [TRANSCRIBE.md](TRANSCRIBE.md) ã‚’å‚ç…§ã€‚

```bash
# macOS / Linux
curl -fsSL https://raw.githubusercontent.com/lmlight-app/dist_v3/main/scripts/install-transcribe.sh | bash
```

```powershell
# Windows
irm https://raw.githubusercontent.com/lmlight-app/dist_v3/main/scripts/install-transcribe.ps1 | iex
```

### ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ (Perpetual License)

**ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æ–¹å¼**: Hardware UUIDãƒ™ãƒ¼ã‚¹æ°¸ç¶šãƒ©ã‚¤ã‚»ãƒ³ã‚¹

- ãƒ‡ãƒã‚¤ã‚¹ã®Hardware UUIDã«ç´ä»˜ã‘ã‚‰ã‚ŒãŸæ°¸ç¶šãƒ©ã‚¤ã‚»ãƒ³ã‚¹
- æœ‰åŠ¹æœŸé™ãªã— (issued_atãƒã‚§ãƒƒã‚¯ãªã—)
- ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ»ã‚ªãƒ³ãƒ—ãƒ¬ãƒŸã‚¹ç’°å¢ƒã§ã®åˆ©ç”¨ã«æœ€é©
- 1ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ = 1ãƒ‡ãƒã‚¤ã‚¹

#### Hardware UUID å–å¾—æ–¹æ³•

ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ç™ºè¡Œã«å¿…è¦ãªHardware UUIDã‚’å–å¾—ã—ã¦ãã ã•ã„ã€‚

**macOS:**
- è¨­å®š â†’ ä¸€èˆ¬ â†’ æƒ…å ± â†’ ã‚·ã‚¹ãƒ†ãƒ ãƒ¬ãƒãƒ¼ãƒˆ â†’ ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ â†’ ã€Œãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢UUIDã€
- ã¾ãŸã¯ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§: `ioreg -d2 -c IOPlatformExpertDevice | awk -F\" '/IOPlatformUUID/{print $4}'`

**Windows:**
- PowerShellã§: `(Get-CimInstance Win32_ComputerSystemProduct).UUID`

**Linux:**
- ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§: `sudo cat /sys/class/dmi/id/product_uuid` ã¾ãŸã¯ `sudo dmidecode -s system-uuid`

#### ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«é…ç½®

`license.lic` ã‚’ä¸‹è¨˜ã«é…ç½®:

- macOS/Linux: `~/.local/lmlight/license.lic`
- Windows: `%LOCALAPPDATA%\lmlight\license.lic`


## èµ·å‹•ãƒ»åœæ­¢

**macOS / Linux:**
```bash
lmlight start   # èµ·å‹•
lmlight stop    # åœæ­¢
```

**Windows:**
```powershell
lmlight start   # èµ·å‹•
lmlight stop    # åœæ­¢
```

â€» è©³ç´°ã¯ [run.md](run.md) ã‚’å‚ç…§

## ã‚¢ã‚¯ã‚»ã‚¹

### ãƒ­ãƒ¼ã‚«ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆåŒã˜PCï¼‰

- Web: http://localhost:3000
- API: http://localhost:8000

### LANã‚¢ã‚¯ã‚»ã‚¹ï¼ˆä»–ã®PCãƒ»ã‚¹ãƒãƒ›ãƒ»ã‚¿ãƒ–ãƒ¬ãƒƒãƒˆï¼‰

èµ·å‹•æ™‚ã«è¡¨ç¤ºã•ã‚Œã‚‹ LAN IP ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼š

```
ğŸŒ LAN access (from other PCs):
   API: http://192.168.1.100:8000
   Web: http://192.168.1.100:3000
```

**IP ã‚¢ãƒ‰ãƒ¬ã‚¹ã®ç¢ºèªæ–¹æ³•:**
- macOS: `ifconfig | grep "inet "`
- Linux: `ip addr show`
- Windows: `ipconfig`

**ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã®è©³ç´°:** [NETWORK.md](NETWORK.md) ã‚’å‚ç…§ã—ã¦ãã ã•ã„

### ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ­ã‚°ã‚¤ãƒ³

`admin@local` / `admin123`

â€» åˆå›ãƒ­ã‚°ã‚¤ãƒ³å¾Œã€ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„

## ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ

åŒã˜ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚³ãƒãƒ³ãƒ‰ã‚’å†å®Ÿè¡Œ (ãƒ‡ãƒ¼ã‚¿ã¯ä¿æŒ)

## ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

**macOS:**
```bash
rm -rf ~/.local/lmlight
sudo rm -f /usr/local/bin/lmlight
```

**Linux:**
```bash
rm -rf ~/.local/lmlight
```

**Windows:**

```powershell
Remove-Item -Recurse -Force "$env:LOCALAPPDATA\lmlight"
```

## ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

```
~/.local/lmlight/
â”œâ”€â”€ api                    # APIãƒã‚¤ãƒŠãƒª (lmlight-perpetual-*)
â”œâ”€â”€ app/                   # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰
â”œâ”€â”€ models/whisper/        # æ–‡å­—èµ·ã“ã—ãƒ¢ãƒ‡ãƒ« (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
â”œâ”€â”€ .env                   # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ license.lic            # ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ (Hardware UUIDãƒ™ãƒ¼ã‚¹)
â”œâ”€â”€ start.sh               # èµ·å‹•
â”œâ”€â”€ stop.sh                # åœæ­¢
â””â”€â”€ logs/                  # ãƒ­ã‚°
```

---

## vLLMãƒãƒ¼ã‚¸ãƒ§ãƒ³


### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« | ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ (Linux ã®ã¿)

```bash
curl -fsSL https://raw.githubusercontent.com/lmlight-app/dist_v3/main/scripts/install-linux-vllm.sh | bash
```

ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å…ˆ: `~/.local/lmlight-vllm`

### å¿…è¦ãªä¾å­˜é–¢ä¿‚

| ä¾å­˜é–¢ä¿‚ | ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« |
|---------|------------|
| Node.js 18+ | `sudo apt install nodejs npm` |
| PostgreSQL 17 + pgvector | `sudo apt install postgresql postgresql-17-pgvector` |
| NVIDIA GPU + CUDA 12.x | [NVIDIA CUDA Toolkit](https://developer.nvidia.com/cuda-downloads) |
| FFmpeg (æ–‡å­—èµ·ã“ã—ç”¨) | `sudo apt install ffmpeg` |
| Tesseract OCR | `sudo apt install tesseract-ocr tesseract-ocr-jpn` |

### è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« (.env)

`~/.local/lmlight-vllm/.env` ã‚’ç·¨é›†:

| ç’°å¢ƒå¤‰æ•° | èª¬æ˜ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ |
|---------|------|-----------|
| `DATABASE_URL` | PostgreSQLæ¥ç¶šURL | `postgresql://lmlight:lmlight@localhost:5432/lmlight` |
| `VLLM_BASE_URL` | vLLMãƒãƒ£ãƒƒãƒˆã‚µãƒ¼ãƒãƒ¼URL | `http://localhost:8080` |
| `VLLM_EMBED_BASE_URL` | vLLMåŸ‹ã‚è¾¼ã¿ã‚µãƒ¼ãƒãƒ¼URL | `http://localhost:8081` |
| `VLLM_VISION_BASE_URL` | vLLM Visionã‚µãƒ¼ãƒãƒ¼ (ä»»æ„) | (ç©º = ãƒãƒ£ãƒƒãƒˆã‚µãƒ¼ãƒãƒ¼ä½¿ç”¨) |
| `VLLM_AUTO_START` | vLLMè‡ªå‹•èµ·å‹• | `true` |
| `VLLM_CHAT_MODEL` | ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ« (HuggingFace ID) | `Qwen/Qwen2.5-1.5B-Instruct` |
| `VLLM_EMBED_MODEL` | åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ« | `intfloat/multilingual-e5-large-instruct` |
| `VLLM_TENSOR_PARALLEL` | ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—GPUæ•° | `1` |
| `VLLM_GPU_MEMORY_UTILIZATION` | GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ | `0.45` |
| `VLLM_MAX_MODEL_LEN` | æœ€å¤§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•· | `4096` |
| `WHISPER_MODEL` | æ–‡å­—èµ·ã“ã—ãƒ¢ãƒ‡ãƒ« (tiny/base/small/medium/large) | `base` |
| `API_HOST` | APIãƒã‚¤ãƒ³ãƒ‰ã‚¢ãƒ‰ãƒ¬ã‚¹ | `0.0.0.0` (å…¨ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹) |
| `API_PORT` | APIãƒãƒ¼ãƒˆ | `8000` |
| `WEB_HOST` | Webãƒã‚¤ãƒ³ãƒ‰ã‚¢ãƒ‰ãƒ¬ã‚¹ | `0.0.0.0` (å…¨ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹) |
| `WEB_PORT` | Webãƒãƒ¼ãƒˆ | `3000` |
| `LICENSE_FILE_PATH` | ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ | `~/.local/lmlight-vllm/license.lic` |

**ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å®š:** Ollamaç‰ˆã¨åŒæ§˜ã«ã€LANå†…ã®ä»–ã®PCã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã§ã™ã€‚è©³ç´°ã¯ [NETWORK.md](NETWORK.md) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

### èµ·å‹•ãƒ»åœæ­¢

```bash
lmlight-vllm start   # èµ·å‹•
lmlight-vllm stop    # åœæ­¢
```

### ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨

**ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã†å ´åˆ**

`.env`ãƒ•ã‚¡ã‚¤ãƒ«ã®`VLLM_CHAT_MODEL`ã¨`VLLM_EMBED_MODEL`ã‚’å¤‰æ›´ï¼š

```bash
# ~/.local/lmlight-vllm/.env

# 1. HuggingFace Hubä¸Šã®ãƒ¢ãƒ‡ãƒ«
VLLM_CHAT_MODEL=username/your-finetuned-model
VLLM_EMBED_MODEL=intfloat/multilingual-e5-large-instruct

# 2. ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼ˆçµ¶å¯¾ãƒ‘ã‚¹ï¼‰
VLLM_CHAT_MODEL=/home/user/models/my-finetuned-llama
VLLM_EMBED_MODEL=/home/user/models/my-finetuned-embeddings

# 3. ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆHuggingFaceãƒ¢ãƒ‡ãƒ«
HF_TOKEN=hf_your_token_here
VLLM_CHAT_MODEL=private-org/private-model
```

**ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã®è¦ä»¶**ï¼šHuggingFace Transformerså½¢å¼ï¼ˆ`config.json`, `tokenizer_config.json`, `model.safetensors`ç­‰ï¼‰

è¨­å®šå¤‰æ›´å¾Œã€å†èµ·å‹•ï¼š
```bash
lmlight-vllm stop
lmlight-vllm start
```

### æ³¨æ„äº‹é …

- **ãƒã‚¤ãƒŠãƒªã‚µã‚¤ã‚º**: APIæœ¬ä½“ã¯ç´„170MBã€‚vLLM/PyTorchã¯åˆ¥é€”venvã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆç´„6GBï¼‰
- **åˆå›èµ·å‹•æ™‚**ã¯HuggingFaceã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆç´„3GBã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¿…è¦ï¼‰
- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾Œã¯ `~/.cache/huggingface/hub/` ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã‚ªãƒ•ãƒ©ã‚¤ãƒ³å‹•ä½œå¯èƒ½
- GPU 1æšã§chat + embedã‚’å‹•ã‹ã™å ´åˆã¯ `VLLM_GPU_MEMORY_UTILIZATION=0.45` ã‚’æ¨å¥¨

### ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

```
~/.local/lmlight-vllm/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ lmlight-vllm-linux-amd64  # å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ« (ç´„170MB)
â”œâ”€â”€ venv/                          # Python venv (vLLM + whisperã€ç´„6GB)
â”œâ”€â”€ app/                           # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰
â”œâ”€â”€ .env                           # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ license.lic                    # ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ (Hardware UUIDãƒ™ãƒ¼ã‚¹)
â”œâ”€â”€ start.sh                       # èµ·å‹•
â”œâ”€â”€ stop.sh                        # åœæ­¢
â”œâ”€â”€ lmlight-vllm                   # CLIã‚³ãƒãƒ³ãƒ‰
â””â”€â”€ logs/                          # ãƒ­ã‚°
```

### Ollamaç‰ˆã¨ã®é•ã„

| é …ç›® | Ollamaç‰ˆ | vLLMç‰ˆ |
|------|---------|--------|
| LLMã‚¨ãƒ³ã‚¸ãƒ³ | Ollama | vLLM |
| GPUè¦ä»¶ | ä»»æ„ (CPUå¯) | NVIDIA GPUå¿…é ˆ |
| ä¸¦åˆ—å‡¦ç† | Ollamaä¾å­˜ | Continuous Batching |
| ãƒ¢ãƒ‡ãƒ«å½¢å¼ | GGUF | HuggingFace |
| å¯¾å¿œOS | macOS/Linux/Windows | Linux |
| ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å…ˆ | `~/.local/lmlight` | `~/.local/lmlight-vllm` |

---

## Dockerç‰ˆ

Docker Compose ã‚’ä½¿ã£ãŸãƒ‡ãƒ—ãƒ­ã‚¤æ–¹æ³•ã§ã™ã€‚PostgreSQL (pgvector) ã‚‚å«ã¾ã‚Œã‚‹ãŸã‚ã€DB ã®å€‹åˆ¥ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¯ä¸è¦ã§ã™ã€‚

### å¿…è¦ãªä¾å­˜é–¢ä¿‚

| ä¾å­˜é–¢ä¿‚ | ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« |
|---------|------------|
| Docker Engine | [Install Docker](https://docs.docker.com/engine/install/) |
| Docker Compose v2 | Docker Engine ã«åŒæ¢± |
| NVIDIA GPU (vLLMç‰ˆã®ã¿) | [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html) |

### å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«

ä»¥ä¸‹ã®3ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®ã—ã¦ãã ã•ã„ã€‚

#### 1. `docker-compose.yml`

```yaml
services:
  postgres:
    image: pgvector/pgvector:pg16
    environment:
      POSTGRES_USER: lmlight
      POSTGRES_PASSWORD: lmlight
      POSTGRES_DB: lmlight
    volumes:
      - pgdata:/var/lib/postgresql/data
    restart: unless-stopped

  api:
    image: lmlight/lmlight-vllm:latest   # Ollamaç‰ˆ: lmlight/lmlight-perpetual:latest
    env_file: .env
    volumes:
      - ./license.lic:/app/license.lic:ro
    ports:
      - "8000:8000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - postgres
    restart: unless-stopped

  app:
    image: lmlight/lmlight-app:latest
    env_file: .env
    ports:
      - "3000:3000"
    depends_on:
      - postgres
      - api
    restart: unless-stopped

  whisper:
    image: onerahmet/openai-whisper-asr-webservice:latest
    environment:
      ASR_MODEL: base
      ASR_ENGINE: openai_whisper
    ports:
      - "9000:9000"
    restart: unless-stopped

volumes:
  pgdata:
```

#### 2. `.env`

**vLLMç‰ˆ:**

```bash
DATABASE_URL=postgresql://lmlight:lmlight@postgres:5432/lmlight

# vLLM ã‚µãƒ¼ãƒãƒ¼ URLï¼ˆä¸Šè¨˜ã® docker run ã‚³ãƒãƒ³ãƒ‰ã§èµ·å‹•ã—ãŸå ´åˆï¼‰
VLLM_BASE_URL=http://host.docker.internal:8080       # Chat ãƒ¢ãƒ‡ãƒ«
VLLM_EMBED_BASE_URL=http://host.docker.internal:8081  # Embedding ãƒ¢ãƒ‡ãƒ«

VLLM_AUTO_START=false
VLLM_CHAT_MODEL=Qwen/Qwen2.5-1.5B-Instruct
VLLM_EMBED_MODEL=intfloat/multilingual-e5-large-instruct
VLLM_TENSOR_PARALLEL=1
VLLM_GPU_MEMORY_UTILIZATION=0.45
# VLLM_MAX_MODEL_LEN=4096

WHISPER_API_URL=http://whisper:9000
API_PORT=8000
API_HOST=0.0.0.0
LICENSE_FILE_PATH=/app/license.lic
NEXT_PUBLIC_API_URL=http://localhost:8000
```

**Ollamaç‰ˆ:**

```
DATABASE_URL=postgresql://lmlight:lmlight@postgres:5432/lmlight
OLLAMA_BASE_URL=http://host.docker.internal:11434
API_PORT=8000
API_HOST=0.0.0.0
LICENSE_FILE_PATH=/app/license.lic
NEXT_PUBLIC_API_URL=http://localhost:8000
```

#### 3. `license.lic`

ç™ºè¡Œã•ã‚ŒãŸãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®ã€‚

### èµ·å‹•ãƒ»åœæ­¢

#### vLLM ç‰ˆ

**1. vLLM ã‚’èµ·å‹•ï¼ˆå…¬å¼ Docker ã‚¤ãƒ¡ãƒ¼ã‚¸ï¼‰**

```bash
# Chat ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒãƒ¼ãƒˆ 8080ï¼‰
docker run -d --name vllm-chat \
  --gpus all \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  -p 8080:8000 \
  --ipc=host \
  vllm/vllm-openai:latest \
  --model Qwen/Qwen2.5-1.5B-Instruct

# Embedding ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒãƒ¼ãƒˆ 8081ï¼‰
docker run -d --name vllm-embed \
  --gpus all \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  -p 8081:8000 \
  --ipc=host \
  vllm/vllm-openai:latest \
  --model intfloat/multilingual-e5-large-instruct \
  --task embed
```

**2. LM Light ã‚’èµ·å‹•**

```bash
docker compose up -d      # èµ·å‹•ï¼ˆåˆå›ã¯ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’è‡ªå‹•pullï¼‰
docker compose logs -f    # ãƒ­ã‚°ç¢ºèª
```

**3. åœæ­¢**

```bash
docker compose down           # LM Light åœæ­¢
docker stop vllm-chat vllm-embed  # vLLM åœæ­¢
docker compose down -v        # LM Light åœæ­¢ + ãƒ‡ãƒ¼ã‚¿å‰Šé™¤
```

#### Ollama ç‰ˆ

**1. Ollama ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ»èµ·å‹•ï¼ˆãƒ›ã‚¹ãƒˆå´ï¼‰**

```bash
# macOS/Linux
brew install ollama  # ã¾ãŸã¯ curl -fsSL https://ollama.com/install.sh | sh
ollama serve &
ollama pull gemma3:4b
ollama pull nomic-embed-text
```

**2. LM Light ã‚’èµ·å‹•**

```bash
docker compose up -d      # èµ·å‹•
docker compose down        # åœæ­¢
docker compose down -v     # åœæ­¢ + ãƒ‡ãƒ¼ã‚¿å‰Šé™¤
```

åˆå›èµ·å‹•æ™‚ã« `app` ã‚³ãƒ³ãƒ†ãƒŠãŒè‡ªå‹•ã§ DB ã‚¹ã‚­ãƒ¼ãƒä½œæˆãƒ»åˆæœŸãƒ¦ãƒ¼ã‚¶ãƒ¼ä½œæˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

### ã‚¢ã‚¯ã‚»ã‚¹

- Web: http://localhost:3000
- API: http://localhost:8000
- åˆå›ãƒ­ã‚°ã‚¤ãƒ³: `admin@local` / `admin123`

### Docker Hub ã‚¤ãƒ¡ãƒ¼ã‚¸ä¸€è¦§

| ã‚¤ãƒ¡ãƒ¼ã‚¸ | èª¬æ˜ |
|---------|------|
| `lmlight/lmlight-vllm:latest` | API (vLLMç‰ˆ) |
| `lmlight/lmlight-perpetual:latest` | API (Ollamaç‰ˆ) |
| `lmlight/lmlight-app:latest` | ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ (å…±é€š) |
| `pgvector/pgvector:pg16` | PostgreSQL + pgvector (å…¬å¼) |

```bash
# ã‚¤ãƒ¡ãƒ¼ã‚¸ã®æ‰‹å‹•pullï¼ˆdocker compose up -d ã§è‡ªå‹•pullã•ã‚Œã¾ã™ãŒã€äº‹å‰å–å¾—ã‚‚å¯èƒ½ï¼‰
docker pull lmlight/lmlight-vllm:latest                        # vLLMç‰ˆ
docker pull lmlight/lmlight-perpetual:latest                   # Ollamaç‰ˆ
docker pull lmlight/lmlight-app:latest                         # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰
docker pull pgvector/pgvector:pg16                             # PostgreSQL
docker pull onerahmet/openai-whisper-asr-webservice:latest     # æ–‡å­—èµ·ã“ã—
```

### æ³¨æ„äº‹é …

**vLLM ç‰ˆ:**
- vLLM ã¯ **å…¬å¼ Docker ã‚¤ãƒ¡ãƒ¼ã‚¸**ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„: `vllm/vllm-openai:latest`
- vLLM ã®è©³ç´°ãªè¨­å®šï¼ˆãƒ¢ãƒ‡ãƒ«é¸æŠã€GPUè¨­å®šãªã©ï¼‰ã¯[å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.vllm.ai/)ã‚’å‚ç…§
- `.env` ã® `VLLM_BASE_URL` / `VLLM_EMBED_BASE_URL` ã¯ãƒãƒ¼ãƒˆã«åˆã‚ã›ã¦è¨­å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 8080, 8081ï¼‰
- Kubernetes ç­‰ã§ vLLM ãŒåˆ¥ Pod ã«ã‚ã‚‹å ´åˆã¯ã€ãã®ã‚µãƒ¼ãƒ“ã‚¹ URL ã‚’æŒ‡å®šã—ã¦ãã ã•ã„

**Ollama ç‰ˆ:**
- Ollama ã¯ãƒ›ã‚¹ãƒˆå´ã§èµ·å‹•ï¼ˆ`host.docker.internal` çµŒç”±ã§æ¥ç¶šï¼‰
- `.env` ã® `OLLAMA_BASE_URL` ã¯ãƒ›ã‚¹ãƒˆå´ã®ã‚µãƒ¼ãƒãƒ¼ã‚¢ãƒ‰ãƒ¬ã‚¹ã«åˆã‚ã›ã¦ãã ã•ã„

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æ¯”è¼ƒ

| é …ç›® | Subscription | Perpetual |
|------|---------------------|---------------------|
| ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯ | æœ‰åŠ¹æœŸé™ | Hardware UUID |
| ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚¿ã‚¤ãƒ— | ã‚µãƒ–ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ | æ°¸ç¶š |
